import React, { useState, useEffect, useRef } from 'react';
import { Button, Flex, Text, VStack, Box } from '@chakra-ui/react';
import { motion, AnimatePresence } from 'framer-motion';
import { useNavigate, useLocation } from 'react-router-dom';

import DabokVideo from '../../video/dabok.webm';
import DajeongVideo from '../../video/dajeung.webm';
import useAppSettings from '../../hooks/useAppSettings';

import { endCall, startCall } from '../../api/callAPI';
import { getAiSocket } from '../../api/aiSocket';

const MotionBox = motion(Flex);
const MotionText = motion(Text);

export default function CallPage() {
    const navigate = useNavigate();
    const location = useLocation();

    const { fontSizeLevel, setFontSizeLevel, isHighContrast, toggleHighContrast, fs, callBtnH } = useAppSettings();

    const [isTalking, setIsTalking] = useState(false);
    const [isUserSpeaking, setIsUserSpeaking] = useState(false);
    const [currentSubtitle, setCurrentSubtitle] = useState('í†µí™” ì—°ê²° ì¤‘...');
    const [aiMessages, setAiMessages] = useState([]);
    const [debugRms, setDebugRms] = useState(0);
    const [vadStatus, setVadStatus] = useState('ëŒ€ê¸° ì¤‘');

    const videoRef = useRef(null);
    const audioStreamRef = useRef(null);
    const audioContextRef = useRef(null);
    const analyserRef = useRef(null);
    const processorRef = useRef(null);
    const audioBufferRef = useRef([]);
    const silenceStartTimeRef = useRef(null);
    const vadStateRef = useRef('idle');
    const aiSpeakingRef = useRef(false);
    const audioChunkCountRef = useRef(0);
    const rmsLogIntervalRef = useRef(0);
    const isCallStartedRef = useRef(false);
    const isRecordingRef = useRef(false);
    const recordingStartTimeRef = useRef(null);

    // VAD ì„¤ì •
    const VAD_THRESHOLD = 0.0001; // 0.00005 â†’ 0.0001 (ì¤‘ê°„ê°’)
    const SILENCE_DURATION = 1500; // 1000ms â†’ 1500ms (1.5ì´ˆ)
    const MIN_RECORDING_TIME = 500; // 300ms â†’ 500ms (0.5ì´ˆ)
    const MIN_AUDIO_CHUNKS = 3; // 1 â†’ 3 (ìµœì†Œ 3ê°œ ì²­í¬)

    const character = location.state?.character || {
        name: 'ë‹¤ì •ì´',
        characterType: 'dajeong',
        color: '#2196F3',
    };

    useEffect(() => {
        const initCall = async () => {
            if (isCallStartedRef.current) {
                console.log('âš ï¸ í†µí™”ê°€ ì´ë¯¸ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ì¤‘ë³µ í˜¸ì¶œ ë°©ì§€');
                return;
            }

            if (location.state) {
                isCallStartedRef.current = true;
                const { character, politeness } = location.state;

                console.log('='.repeat(50));
                console.log('ğŸ¬ í†µí™” ì´ˆê¸°í™” ì‹œì‘');
                console.log('='.repeat(50));

                await startCall(character, politeness);
                setupWebSocketHandler();
                startMicrophone();

                console.log('âœ… í†µí™” ì´ˆê¸°í™” ì™„ë£Œ');
                console.log('='.repeat(50));
            }
        };

        initCall();

        return () => {
            console.log('ğŸ§¹ CallPage cleanup ì‹œì‘');
            stopMicrophone();
        };
    }, []);

    const startMicrophone = async () => {
        try {
            console.log('ğŸ¤ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì¤‘...');

            const stream = await navigator.mediaDevices.getUserMedia({
                audio: {
                    echoCancellation: true,
                    noiseSuppression: true,
                    autoGainControl: true,
                },
            });
            audioStreamRef.current = stream;

            const audioContext = new (window.AudioContext || window.webkitAudioContext)();
            audioContextRef.current = audioContext;

            console.log('ğŸ”Š AudioContext ìƒ˜í”Œë ˆì´íŠ¸:', audioContext.sampleRate, 'Hz');

            const source = audioContext.createMediaStreamSource(stream);

            const gainNode = audioContext.createGain();
            gainNode.gain.value = 2.0; // 3.0 â†’ 2.0 (ì¡°ê¸ˆ ë‚®ì¶¤)
            console.log('ğŸ”Š ë§ˆì´í¬ ê²Œì¸ ì„¤ì •:', gainNode.gain.value, 'x');

            const analyser = audioContext.createAnalyser();
            analyser.fftSize = 2048;
            analyserRef.current = analyser;

            const processor = audioContext.createScriptProcessor(4096, 1, 1);
            processorRef.current = processor;

            processor.onaudioprocess = (e) => {
                const inputData = e.inputBuffer.getChannelData(0);

                // RMS ê³„ì‚°
                let sum = 0;
                for (let i = 0; i < inputData.length; i++) {
                    sum += inputData[i] * inputData[i];
                }
                const rms = Math.sqrt(sum / inputData.length);

                // UIì— RMS í‘œì‹œ
                setDebugRms(rms);

                // ë¡œê¹… (10ë²ˆë§ˆë‹¤ - ë” ìì£¼)
                rmsLogIntervalRef.current++;
                if (rmsLogIntervalRef.current % 10 === 0) {
                    console.log(
                        `ğŸ“Š RMS: ${rms.toFixed(7)} | ì„ê³„ê°’: ${VAD_THRESHOLD} | AI: ${aiSpeakingRef.current} | VAD: ${
                            vadStateRef.current
                        } | ë…¹ìŒ: ${isRecordingRef.current} | ì²­í¬: ${audioChunkCountRef.current}`
                    );
                }

                // AIê°€ ë§í•˜ëŠ” ì¤‘ì´ë©´ VAD ë¹„í™œì„±í™”
                if (aiSpeakingRef.current) {
                    if (vadStateRef.current !== 'idle') {
                        console.log('ğŸ¤– AI ë§í•˜ëŠ” ì¤‘ - VAD ë¹„í™œì„±í™”');
                        vadStateRef.current = 'idle';
                        silenceStartTimeRef.current = null;
                        setIsUserSpeaking(false);
                        setVadStatus('AI ë§í•˜ëŠ” ì¤‘');

                        if (isRecordingRef.current) {
                            console.log('ğŸ›‘ AI ë§í•˜ëŠ” ì¤‘ - ë…¹ìŒ ê°•ì œ ì¢…ë£Œ');
                            sendStopMessage();
                            audioBufferRef.current = [];
                            audioChunkCountRef.current = 0;
                            recordingStartTimeRef.current = null;
                        }
                    }
                    return;
                }

                const now = Date.now();

                if (rms > VAD_THRESHOLD) {
                    // ìŒì„± ê°ì§€
                    if (vadStateRef.current === 'idle') {
                        console.log('='.repeat(50));
                        console.log('ğŸ¤ ìŒì„± ê°ì§€ ì‹œì‘!');
                        console.log('   RMS ê°’:', rms.toFixed(7));
                        console.log('   ì„ê³„ê°’:', VAD_THRESHOLD);
                        console.log('='.repeat(50));

                        sendStartMessage();

                        vadStateRef.current = 'speaking';
                        setIsUserSpeaking(true);
                        setVadStatus('ğŸ¤ ë…¹ìŒ ì¤‘...');
                        audioBufferRef.current = [];
                        audioChunkCountRef.current = 0;
                        recordingStartTimeRef.current = now;
                    }

                    // ì¹¨ë¬µì—ì„œ ë‹¤ì‹œ ìŒì„± ê°ì§€
                    if (silenceStartTimeRef.current !== null) {
                        const wasSilent = vadStateRef.current === 'silence';
                        const interruptedSilenceDuration = now - silenceStartTimeRef.current;
                        silenceStartTimeRef.current = null;

                        if (wasSilent) {
                            console.log(`ğŸ¤ ì¹¨ë¬µ ì¤‘ë‹¨ (${interruptedSilenceDuration}ms ë§Œì—) - ê³„ì† ë…¹ìŒ`);
                            vadStateRef.current = 'speaking';
                            setVadStatus('ğŸ¤ ë…¹ìŒ ì¤‘...');
                        }
                    }

                    // PCM16 ë³€í™˜ (16kHz ë‹¤ìš´ìƒ˜í”Œë§)
                    const downsampleRatio = Math.round(audioContext.sampleRate / 16000);
                    const downsampledLength = Math.floor(inputData.length / downsampleRatio);
                    const downsampledData = new Float32Array(downsampledLength);

                    for (let i = 0; i < downsampledLength; i++) {
                        downsampledData[i] = inputData[i * downsampleRatio];
                    }

                    const int16Data = new Int16Array(downsampledLength);
                    for (let i = 0; i < downsampledLength; i++) {
                        const s = Math.max(-1, Math.min(1, downsampledData[i]));
                        int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
                    }

                    // âœ… ì‹¤ì‹œê°„ ì „ì†¡ (HTML ì˜ˆì œì²˜ëŸ¼)
                    if (isRecordingRef.current) {
                        const socket = getAiSocket();
                        if (socket && socket.readyState === WebSocket.OPEN) {
                            socket.send(int16Data.buffer);
                        }
                    }

                    // ë¡œì»¬ ë²„í¼ì—ë„ ì €ì¥ (ë””ë²„ê¹…/ë¡œê·¸ìš©)
                    audioBufferRef.current.push(int16Data);
                    audioChunkCountRef.current++;

                    if (audioChunkCountRef.current === 1 || audioChunkCountRef.current % 10 === 0) {
                        console.log(`ğŸ”Š ì²­í¬ ì‹¤ì‹œê°„ ì „ì†¡ ì¤‘: ${audioChunkCountRef.current}ê°œ`);
                    }
                } else {
                    // ì¹¨ë¬µ ê°ì§€
                    if (vadStateRef.current === 'speaking') {
                        if (silenceStartTimeRef.current === null) {
                            silenceStartTimeRef.current = now;
                            vadStateRef.current = 'silence';
                            console.log('='.repeat(50));
                            console.log('ğŸ”‡ ì¹¨ë¬µ ê°ì§€ - ëŒ€ê¸° ì‹œì‘');
                            console.log('   í˜„ì¬ ë…¹ìŒ ìƒíƒœ:', isRecordingRef.current);
                            console.log('   í˜„ì¬ ì²­í¬ ìˆ˜:', audioChunkCountRef.current);
                            console.log('   ë…¹ìŒ ì‹œì‘ ì‹œê°„:', recordingStartTimeRef.current);
                            console.log('='.repeat(50));
                            setVadStatus('â¸ï¸ ì¹¨ë¬µ ê°ì§€ ì¤‘...');
                        }
                    }

                    // ì¹¨ë¬µ ì§€ì† ì‹œê°„ ì²´í¬
                    if (vadStateRef.current === 'silence' && silenceStartTimeRef.current !== null) {
                        const silenceDuration = now - silenceStartTimeRef.current;

                        // 100msë§ˆë‹¤ ë¡œê·¸ (ë” ìì£¼)
                        if (Math.floor(silenceDuration / 100) !== Math.floor((silenceDuration - 50) / 100)) {
                            console.log(
                                `â±ï¸ ì¹¨ë¬µ ${silenceDuration}ms / ${SILENCE_DURATION}ms (ì²­í¬: ${audioChunkCountRef.current}, ë…¹ìŒì¤‘: ${isRecordingRef.current})`
                            );
                        }

                        if (silenceDuration >= SILENCE_DURATION) {
                            // ë…¹ìŒ ì‹œê°„ ì²´í¬
                            const recordingDuration = recordingStartTimeRef.current
                                ? now - recordingStartTimeRef.current
                                : 0;

                            console.log('='.repeat(50));
                            console.log('ğŸ“¤ ì¹¨ë¬µ ì§€ì† - ë…¹ìŒ ì¢…ë£Œ íŒë‹¨');
                            console.log('   ì¹¨ë¬µ ì‹œê°„:', silenceDuration, 'ms');
                            console.log('   ë…¹ìŒ ì‹œê°„:', recordingDuration, 'ms');
                            console.log('   ì²­í¬ ìˆ˜:', audioChunkCountRef.current);
                            console.log('   ë…¹ìŒ ìƒíƒœ:', isRecordingRef.current);
                            console.log('   ìµœì†Œ ì¡°ê±´: ë…¹ìŒ', MIN_RECORDING_TIME, 'ms, ì²­í¬', MIN_AUDIO_CHUNKS, 'ê°œ');
                            console.log('   ì¡°ê±´ ì²´í¬:');
                            console.log(
                                '     - isRecording:',
                                isRecordingRef.current,
                                isRecordingRef.current ? 'âœ…' : 'âŒ'
                            );
                            console.log(
                                '     - recordingDuration >= MIN:',
                                recordingDuration >= MIN_RECORDING_TIME,
                                recordingDuration >= MIN_RECORDING_TIME ? 'âœ…' : 'âŒ'
                            );
                            console.log(
                                '     - chunkCount >= MIN:',
                                audioChunkCountRef.current >= MIN_AUDIO_CHUNKS,
                                audioChunkCountRef.current >= MIN_AUDIO_CHUNKS ? 'âœ…' : 'âŒ'
                            );
                            console.log('='.repeat(50));

                            // ìµœì†Œ ë…¹ìŒ ì‹œê°„ ë° ì²­í¬ ìˆ˜ ì²´í¬
                            if (
                                isRecordingRef.current &&
                                recordingDuration >= MIN_RECORDING_TIME &&
                                audioChunkCountRef.current >= MIN_AUDIO_CHUNKS
                            ) {
                                console.log('âœ… ëª¨ë“  ì¡°ê±´ ë§Œì¡± - ë…¹ìŒ ì¢…ë£Œ, ì„œë²„ë¡œ ì „ì†¡');
                                sendStopMessage();
                                setVadStatus('ğŸ“¤ ì „ì†¡ ì¤‘...');
                            } else {
                                console.log('âš ï¸ ì¡°ê±´ ë¯¸ì¶©ì¡± - ë…¹ìŒì´ ë„ˆë¬´ ì§§ê±°ë‚˜ ë°ì´í„° ì—†ìŒ');
                                isRecordingRef.current = false;
                                setVadStatus('âš ï¸ ë„ˆë¬´ ì§§ìŒ (ë‹¤ì‹œ ë§ì”€í•´ì£¼ì„¸ìš”)');

                                // 2ì´ˆ í›„ ìƒíƒœ ë³µêµ¬
                                setTimeout(() => {
                                    if (vadStateRef.current === 'idle') {
                                        setVadStatus('ëŒ€ê¸° ì¤‘');
                                    }
                                }, 2000);
                            }

                            vadStateRef.current = 'idle';
                            setIsUserSpeaking(false);
                            audioBufferRef.current = [];
                            audioChunkCountRef.current = 0;
                            silenceStartTimeRef.current = null;
                            recordingStartTimeRef.current = null;
                        }
                    }
                }
            };

            source.connect(gainNode);
            gainNode.connect(analyser);
            analyser.connect(processor);
            processor.connect(audioContext.destination);

            console.log('='.repeat(50));
            console.log('âœ… ë§ˆì´í¬ ì‹œì‘ ì™„ë£Œ (ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹)');
            console.log('   ìƒ˜í”Œë ˆì´íŠ¸:', audioContext.sampleRate, 'Hz');
            console.log('   VAD ì„ê³„ê°’:', VAD_THRESHOLD, '(ë†’ìŒ - ëª…í™•í•œ ìŒì„±ë§Œ)');
            console.log('   ì¹¨ë¬µ ì‹œê°„:', SILENCE_DURATION, 'ms (ì§§ìŒ - ë¹ ë¥¸ ì‘ë‹µ)');
            console.log('   ìµœì†Œ ë…¹ìŒ ì‹œê°„:', MIN_RECORDING_TIME, 'ms');
            console.log('   ìµœì†Œ ì²­í¬ ìˆ˜:', MIN_AUDIO_CHUNKS);
            console.log('   ë§ˆì´í¬ ê²Œì¸:', gainNode.gain.value, 'x');
            console.log('   ì „ì†¡ ë°©ì‹: ì‹¤ì‹œê°„ ìŠ¤íŠ¸ë¦¬ë° (ì²­í¬ë§ˆë‹¤ ì¦‰ì‹œ ì „ì†¡)');
            console.log('='.repeat(50));

            setVadStatus('ëŒ€ê¸° ì¤‘');
        } catch (error) {
            console.error('âŒ ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­ ì‹¤íŒ¨:', error);
            alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤. ë¸Œë¼ìš°ì € ì„¤ì •ì—ì„œ ë§ˆì´í¬ ê¶Œí•œì„ í—ˆìš©í•´ì£¼ì„¸ìš”.');
            setVadStatus('âŒ ë§ˆì´í¬ ì˜¤ë¥˜');
        }
    };

    const sendStartMessage = () => {
        const socket = getAiSocket();
        if (!socket || socket.readyState !== WebSocket.OPEN) {
            console.error('âŒ WebSocket ì—°ê²° ì•ˆ ë¨');
            return;
        }

        if (isRecordingRef.current) {
            console.log('âš ï¸ ì´ë¯¸ ë…¹ìŒ ì¤‘ - start ì „ì†¡ ìŠ¤í‚µ');
            return;
        }

        try {
            const startMsg = {
                type: 'start',
                lang: 'ko',
            };
            socket.send(JSON.stringify(startMsg));
            isRecordingRef.current = true;
            console.log('ğŸ“¤ START ë©”ì‹œì§€ ì „ì†¡ ì™„ë£Œ:', startMsg);
            console.log('   ë…¹ìŒ ìƒíƒœë¥¼ trueë¡œ ë³€ê²½');
        } catch (error) {
            console.error('âŒ ì‹œì‘ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨:', error);
        }
    };

    const sendStopMessage = () => {
        const socket = getAiSocket();
        if (!socket || socket.readyState !== WebSocket.OPEN) {
            console.error('âŒ WebSocket ì—°ê²° ì•ˆ ë¨');
            return;
        }

        if (!isRecordingRef.current) {
            console.log('âš ï¸ ë…¹ìŒ ì¤‘ì´ ì•„ë‹˜ - stop ì „ì†¡ ìŠ¤í‚µ');
            return;
        }

        try {
            console.log('='.repeat(50));
            console.log('ğŸ“¤ STOP ë©”ì‹œì§€ ì „ì†¡');
            console.log('   ì´ ì „ì†¡ëœ ì²­í¬ ìˆ˜:', audioBufferRef.current.length);
            console.log(
                '   ì´ ìƒ˜í”Œ ìˆ˜:',
                audioBufferRef.current.reduce((sum, chunk) => sum + chunk.length, 0)
            );

            // stop ë©”ì‹œì§€ë§Œ ì „ì†¡ (ì˜¤ë””ì˜¤ëŠ” ì´ë¯¸ ì‹¤ì‹œê°„ìœ¼ë¡œ ì „ì†¡ë¨)
            const stopMsg = {
                type: 'stop',
            };
            socket.send(JSON.stringify(stopMsg));
            console.log('   âœ… stop JSON ì „ì†¡ ì™„ë£Œ');

            isRecordingRef.current = false;
            console.log('   ë…¹ìŒ ìƒíƒœë¥¼ falseë¡œ ë³€ê²½');
            console.log('='.repeat(50));
        } catch (error) {
            console.error('âŒ ì¢…ë£Œ ë©”ì‹œì§€ ì „ì†¡ ì‹¤íŒ¨:', error);
        }
    };

    const stopMicrophone = () => {
        console.log('='.repeat(50));
        console.log('ğŸ¤ ë§ˆì´í¬ ì¤‘ì§€ ì‹œì‘...');

        if (isRecordingRef.current) {
            sendStopMessage();
        }

        silenceStartTimeRef.current = null;
        recordingStartTimeRef.current = null;

        if (processorRef.current) {
            try {
                processorRef.current.disconnect();
                processorRef.current.onaudioprocess = null;
                processorRef.current = null;
            } catch (e) {
                console.warn('   âš ï¸ ScriptProcessor ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:', e);
            }
        }

        if (analyserRef.current) {
            try {
                analyserRef.current.disconnect();
                analyserRef.current = null;
            } catch (e) {
                console.warn('   âš ï¸ Analyser ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:', e);
            }
        }

        if (audioContextRef.current) {
            try {
                audioContextRef.current.close();
                audioContextRef.current = null;
            } catch (e) {
                console.warn('   âš ï¸ AudioContext ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:', e);
            }
        }

        if (audioStreamRef.current) {
            try {
                audioStreamRef.current.getTracks().forEach((track) => track.stop());
                audioStreamRef.current = null;
            } catch (e) {
                console.warn('   âš ï¸ ì˜¤ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ ì •ë¦¬ ì¤‘ ì˜¤ë¥˜:', e);
            }
        }

        vadStateRef.current = 'idle';
        audioBufferRef.current = [];
        audioChunkCountRef.current = 0;
        rmsLogIntervalRef.current = 0;
        isRecordingRef.current = false;
        setIsUserSpeaking(false);
        setVadStatus('ì¢…ë£Œë¨');

        console.log('âœ… ë§ˆì´í¬ ì¤‘ì§€ ì™„ë£Œ');
        console.log('='.repeat(50));
    };

    useEffect(() => {
        if (!videoRef.current) return;

        if (isTalking) {
            videoRef.current.play().catch((e) => {
                console.log('Video play failed:', e);
            });
        } else {
            videoRef.current.pause();
        }
    }, [isTalking]);

    const setupWebSocketHandler = () => {
        const socket = getAiSocket();
        if (!socket) {
            console.error('âŒ WebSocketì´ ì—†ìŠµë‹ˆë‹¤. í•¸ë“¤ëŸ¬ ë“±ë¡ ì‹¤íŒ¨');
            return;
        }

        console.log('='.repeat(50));
        console.log('ğŸ“¡ WebSocket ë©”ì‹œì§€ í•¸ë“¤ëŸ¬ ë“±ë¡');
        console.log('='.repeat(50));

        socket.onmessage = async (event) => {
            const data = event.data;

            if (data instanceof Blob) {
                console.log('='.repeat(50));
                console.log('ğŸ“¥ AI ì˜¤ë””ì˜¤ Blob ìˆ˜ì‹ ');
                console.log('   í¬ê¸°:', data.size, 'bytes');

                if (data.size < 100) {
                    console.log('âš ï¸ ì˜¤ë””ì˜¤ í¬ê¸°ê°€ ë„ˆë¬´ ì‘ìŒ');
                    return;
                }

                const audioUrl = URL.createObjectURL(data);
                const audio = new Audio(audioUrl);

                setIsTalking(true);
                aiSpeakingRef.current = true;
                setVadStatus('ğŸ¤– AI ë§í•˜ëŠ” ì¤‘');
                console.log('ğŸ”Š AI ë§í•˜ê¸° ì‹œì‘');

                audio.onended = () => {
                    setIsTalking(false);
                    aiSpeakingRef.current = false;
                    setVadStatus('ëŒ€ê¸° ì¤‘');
                    URL.revokeObjectURL(audioUrl);
                    console.log('âœ… AI ë§í•˜ê¸° ì¢…ë£Œ');
                    console.log('='.repeat(50));
                };

                audio.onerror = (error) => {
                    console.error('âŒ ì˜¤ë””ì˜¤ ì¬ìƒ ì‹¤íŒ¨:', error);
                    setIsTalking(false);
                    aiSpeakingRef.current = false;
                    setVadStatus('ëŒ€ê¸° ì¤‘');
                    URL.revokeObjectURL(audioUrl);
                };

                try {
                    await audio.play();
                    console.log('âœ… ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘');
                } catch (error) {
                    console.error('âŒ audio.play() ì‹¤íŒ¨:', error);
                    setIsTalking(false);
                    aiSpeakingRef.current = false;
                    setVadStatus('ëŒ€ê¸° ì¤‘');
                }

                return;
            }

            try {
                const msg = JSON.parse(data);
                const msgType = msg.type || 'unknown';
                console.log('ğŸ“© AI JSON ë©”ì‹œì§€ ìˆ˜ì‹ :', msgType, msg);

                setAiMessages((prev) => [...prev, msg]);

                if (msg.type === 'ready' && msg.event === 'start') {
                    console.log('âœ… ë°±ì—”ë“œ ë…¹ìŒ ì¤€ë¹„ ì™„ë£Œ');
                } else if (msg.type === 'ended' && msg.event === 'stop') {
                    console.log('âœ… ë°±ì—”ë“œ ë…¹ìŒ ì¢…ë£Œ - AI ì‘ë‹µ ëŒ€ê¸°');
                    setVadStatus('ğŸ¤– AI ìƒê° ì¤‘...');
                } else if (msg.type === 'tts_start' && msg.text) {
                    setCurrentSubtitle(msg.text);
                    console.log('   ìë§‰:', msg.text);
                } else if (msg.type === 'tts_end') {
                    console.log('   TTS ì¢…ë£Œ');
                } else if (msg.type === 'stt_status') {
                    if (msg.message) {
                        setCurrentSubtitle(msg.message);
                        console.log('   STT:', msg.message);
                    }
                } else if (msg.type === 'status') {
                    if (msg.message) {
                        setCurrentSubtitle(msg.message);
                        console.log('   ìƒíƒœ:', msg.message);
                    }
                } else if (msg.type === 'error') {
                    console.error('âŒ ì„œë²„ ì—ëŸ¬:', msg.message);
                    setCurrentSubtitle(msg.message || 'ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤');
                }
            } catch (err) {
                console.warn('âš ï¸ JSON íŒŒì‹± ì‹¤íŒ¨:', data);
            }
        };

        socket.onerror = (error) => {
            console.error('âŒ WebSocket ì—ëŸ¬:', error);
        };

        socket.onclose = (event) => {
            console.log('ğŸ”Œ WebSocket ì—°ê²° ì¢…ë£Œ');
        };
    };

    const handleEndCall = () => {
        console.log('ğŸ“ í†µí™” ì¢…ë£Œ ìš”ì²­');
        stopMicrophone();
        endCall();
        setIsTalking(false);
        console.log('âœ… í†µí™” ì¢…ë£Œ ì™„ë£Œ');
        navigate('/app/home');
    };

    return (
        <Flex minH="100vh" align="center" justify="center" bg={isHighContrast ? '#000000' : 'white'} px={3}>
            <Box p={{ base: 5, md: 14 }} w="full" maxW="530px">
                <VStack spacing={6} align="stretch">
                    <MotionBox
                        initial={{ scale: 0.8, opacity: 0 }}
                        animate={{ scale: 1, opacity: 1 }}
                        transition={{ duration: 0.5 }}
                        w="100%"
                        h="450px"
                        display="flex"
                        alignItems="center"
                        justifyContent="center"
                        overflow="hidden"
                        borderRadius="15px"
                    >
                        <Box
                            as="video"
                            ref={videoRef}
                            src={character.characterType === 'dabok' ? DabokVideo : DajeongVideo}
                            loop
                            muted
                            playsInline
                            w="100%"
                            h="70%"
                            objectFit="cover"
                        />
                    </MotionBox>

                    {/* ìŒì„± ê°ì§€ ìƒíƒœ í‘œì‹œ */}
                    <Box
                        bg={
                            vadStatus.includes('ë…¹ìŒ')
                                ? 'red.500'
                                : vadStatus.includes('ì¹¨ë¬µ')
                                ? 'orange.400'
                                : vadStatus.includes('AI')
                                ? 'blue.500'
                                : vadStatus.includes('ì „ì†¡')
                                ? 'green.500'
                                : vadStatus.includes('ì˜¤ë¥˜') || vadStatus.includes('ì§§ìŒ')
                                ? 'red.600'
                                : 'gray.400'
                        }
                        color="white"
                        px={6}
                        py={4}
                        borderRadius="15px"
                        textAlign="center"
                    >
                        <Text fontSize="2xl" fontWeight="bold">
                            {vadStatus}
                        </Text>
                        <Text fontSize="sm" mt={1}>
                            RMS: {debugRms.toFixed(7)} | ì„ê³„ê°’: {VAD_THRESHOLD}
                        </Text>
                    </Box>

                    <Box mt={2}>
                        <AnimatePresence mode="wait">
                            <MotionText
                                key={currentSubtitle}
                                initial={{ opacity: 0, y: 20 }}
                                animate={{ opacity: 1, y: 0 }}
                                exit={{ opacity: 0, y: -20 }}
                                transition={{ duration: 0.3 }}
                                fontSize={fs}
                                fontWeight="700"
                                color={isHighContrast ? '#FFFFFF' : '#000000'}
                                textAlign="center"
                                py={5}
                                borderRadius="15px"
                                minH="90px"
                                display="flex"
                                alignItems="center"
                                justifyContent="center"
                                w="full"
                            >
                                {currentSubtitle}
                            </MotionText>
                        </AnimatePresence>
                    </Box>

                    <Button
                        w="full"
                        bg={isHighContrast ? '#FFD700' : '#F44336'}
                        color={isHighContrast ? '#000000' : 'white'}
                        onClick={handleEndCall}
                        fontSize={fs}
                        fontWeight="700"
                        height={callBtnH}
                        borderRadius="15px"
                        border={isHighContrast ? '3px solid white' : 'none'}
                        boxShadow="0 4px 14px rgba(244, 67, 54, 0.3)"
                        mt={2}
                        _hover={{
                            bg: isHighContrast ? '#FFEB3B' : '#D32F2F',
                            transform: 'translateY(-2px)',
                            boxShadow: isHighContrast
                                ? '0 6px 20px rgba(255, 215, 0, 0.4)'
                                : '0 6px 20px rgba(244, 67, 54, 0.4)',
                        }}
                        _active={{
                            bg: isHighContrast ? '#FFC107' : '#C62828',
                            transform: 'translateY(0)',
                        }}
                        transition="all 0.2s"
                    >
                        í†µí™” ì¢…ë£Œ
                    </Button>
                </VStack>
            </Box>
        </Flex>
    );
}
