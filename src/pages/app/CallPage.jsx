import React, { useState, useEffect, useRef } from 'react';
import { Button, Flex, Text, VStack, Box, Image, Divider, IconButton, useToast } from '@chakra-ui/react';
import { motion, AnimatePresence } from 'framer-motion';
import { useNavigate, useLocation } from 'react-router-dom';
import { FaMicrophone, FaStop } from 'react-icons/fa';

import DabokVideo from '../../video/dabok.webm';
import DajeongVideo from '../../video/dajeung.webm';
import useAppSettings from '../../hooks/useAppSettings';
import useAudioRecorder from '../../hooks/useAudioRecorder';

import { endCall, startCall } from '../../api/callAPI';
import { getAiSocket } from '../../api/aiSocket';

const MotionBox = motion(Flex);
const MotionText = motion(Text);

export default function CallPage() {
    const navigate = useNavigate();
    const location = useLocation();
    const toast = useToast();

    const { fontSizeLevel, setFontSizeLevel, isHighContrast, toggleHighContrast, fs, callBtnH } = useAppSettings();

    const [isTalking, setIsTalking] = useState(false); // AIê°€ ë§í•˜ëŠ” ì¤‘
    const [isUserTalking, setIsUserTalking] = useState(false); // ì‚¬ìš©ìê°€ ë§í•˜ëŠ” ì¤‘
    const [currentSubtitle, setCurrentSubtitle] = useState('');
    const [userText, setUserText] = useState(''); // ì‚¬ìš©ì ìŒì„±ì¸ì‹ í…ìŠ¤íŠ¸
    const [aiText, setAiText] = useState(''); // AI ì‘ë‹µ í…ìŠ¤íŠ¸
    const [isCallActive, setIsCallActive] = useState(false); // í†µí™” í™œì„±í™” ìƒíƒœ

    const videoRef = useRef(null); // video íƒœê·¸ ref
    const audioRef = useRef(null); // TTS ì˜¤ë””ì˜¤ ì¬ìƒìš© ref

    // ì˜¤ë””ì˜¤ ë…¹ìŒ í›…
    const { isRecording, error: recordError, toggleRecording } = useAudioRecorder();

    // ì „ë‹¬ë°›ì€ ìºë¦­í„° ì •ë³´
    const character = location.state?.character || {
        name: 'ë‹¤ì •ì´',
        characterType: 'dajeong',
        color: '#2196F3',
    };

    const politeness = location.state?.politeness || 'jondae';

    // ì˜¤ë””ì˜¤ ì¬ìƒ í•¨ìˆ˜
    const playAudio = useCallback(async (audioData) => {
        try {
            const arrayBuffer =
                audioData instanceof Blob
                    ? await audioData.arrayBuffer()
                    : audioData instanceof ArrayBuffer
                    ? audioData
                    : audioData.buffer;
            const size = arrayBuffer.byteLength;
            console.log(`ğŸµ ì¬ìƒ ì‹œì‘: ${size} bytes`);

            if (!audioContextRef.current) {
                audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
                    sampleRate: 16000,
                });
            }

            if (audioContextRef.current.state === 'suspended') {
                await audioContextRef.current.resume();
                console.log(`ğŸ”Š AudioContext ì¬ê°œë¨`);
            }

            if (arrayBuffer.byteLength < 2) {
                console.warn(`âš ï¸ ì˜¤ë””ì˜¤ ë°ì´í„°ê°€ ë„ˆë¬´ ì‘ìŒ: ${arrayBuffer.byteLength} bytes`);
                return;
            }

            const audioDataInt16 = new Int16Array(arrayBuffer);
            const float32Array = new Float32Array(audioDataInt16.length);

            for (let i = 0; i < audioDataInt16.length; i++) {
                float32Array[i] = audioDataInt16[i] / 32768.0;
            }

            const audioBuffer = audioContextRef.current.createBuffer(1, float32Array.length, 16000);
            audioBuffer.getChannelData(0).set(float32Array);

            const source = audioContextRef.current.createBufferSource();
            source.buffer = audioBuffer;
            source.connect(audioContextRef.current.destination);

            source.onended = () => {
                console.log(`âœ… ì¬ìƒ ì™„ë£Œ (${audioBuffer.duration.toFixed(2)}ì´ˆ)`);
                setIsTalking(false);
            };

            source.start(0);
            console.log(`â–¶ï¸ ì¬ìƒ ì‹œì‘ë¨ (${audioBuffer.duration.toFixed(2)}ì´ˆ)`);
            setIsTalking(true);
        } catch (error) {
            console.error('âŒ ì¬ìƒ ì˜¤ë¥˜:', error);
        }
    }, []);

    // WebSocket ë©”ì‹œì§€ í•¸ë“¤ëŸ¬
    const handleMessage = useCallback((data) => {
        console.log(`ğŸ“¨ ìˆ˜ì‹ : ${data.type || data.event || 'unknown'}`, data);

        switch (data.type || data.event) {
            case 'start':
                console.log(`ğŸ“ í†µí™” ì‹œì‘: ${data.message || 'í†µí™” ê±°ëŠ” ì¤‘...'}`);
                setCurrentSubtitle(data.message || 'í†µí™” ê±°ëŠ” ì¤‘...');
                break;
            case 'system':
                console.log(`ğŸ’¬ ì‹œìŠ¤í…œ: ${data.message || ''}`);
                break;
            case 'ready':
                if (data.event === 'start') {
                    console.log('âœ… ìŒì„± ë…¹ìŒ ì¤€ë¹„ ì™„ë£Œ');
                }
                break;
            case 'ended':
                if (data.event === 'stop') {
                    console.log('âœ… ìŒì„± ë…¹ìŒ ì¢…ë£Œ ì™„ë£Œ');
                }
                break;
            case 'status':
                console.log(`ğŸ“Š ìƒíƒœ: ${data.message}`);
                setCurrentSubtitle(data.message || '');
                break;
            case 'stt_status':
                console.log(`ğŸ™ï¸ STT ì§„í–‰ ì¤‘: ${data.message}`);
                setCurrentSubtitle('ìŒì„±ì„ ë¶„ì„ ì¤‘ì…ë‹ˆë‹¤...');
                break;
            case 'tts_start':
                console.log(`ğŸ”Š TTS ì‹œì‘: "${data.text}"`);
                setCurrentSubtitle(data.text || '');
                setIsTalking(true);
                break;
            case 'tts_end':
                console.log(`ğŸ”Š TTS ì¢…ë£Œ`);
                setIsTalking(false);
                break;
            case 'transcription':
                setTranscriptions((prev) => [
                    ...prev,
                    { type: 'user', text: data.user_text },
                    { type: 'assistant', text: data.assistant_text },
                ]);
                setCurrentSubtitle(data.assistant_text || '');
                break;
            case 'call_summary':
                console.log('ğŸ“Š í†µí™” ìš”ì•½:', data);
                console.log('ğŸ“Š ê°ì • í†µê³„:', data.emotion_statistics);
                console.log('ğŸ“ ëŒ€í™” ìš”ì•½:', data.conversation_summary);
                // TODO: í†µí™” ìš”ì•½ ë°ì´í„°ë¥¼ ì €ì¥í•˜ê±°ë‚˜ í‘œì‹œ
                setCurrentSubtitle('í†µí™”ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
                break;
            case 'auto_disconnect':
                console.warn(`âš ï¸ ë¹„ì •ìƒ ì¢…ë£Œ: ${data.message}`);
                setCurrentSubtitle('í†µí™”ê°€ ë¹„ì •ìƒì ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.');
                break;
            case 'error':
                console.error(`âŒ ì˜¤ë¥˜: ${data.message}`);
                if (data.message === 'no active session') {
                    console.error('ë…¹ìŒ ì„¸ì…˜ì´ í™œì„±í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.');
                } else {
                    setCurrentSubtitle(`ì˜¤ë¥˜: ${data.message}`);
                }
                break;
        }
    }, []);

    // ìŒì„± ì—ë„ˆì§€ ê³„ì‚°
    const calculateEnergy = (audioData) => {
        let sum = 0;
        for (let i = 0; i < audioData.length; i++) {
            sum += audioData[i] * audioData[i];
        }
        return Math.sqrt(sum / audioData.length);
    };

    // ìŒì„± ê°ì§€
    const detectSpeech = (audioData) => {
        const energy = calculateEnergy(audioData);
        return energy > VAD_CONFIG.energyThreshold;
    };

    // í†µí™” ì‹œì‘ - ë§ˆì´í¬ ì´ˆê¸°í™”
    useEffect(() => {
        const initCall = async () => {
            if (!location.state) return;

            try {
                // 1. í†µí™” ì‹œì‘ API í˜¸ì¶œ
                try {
                    await startCall(character, politeness);
                    console.log('ğŸ“ í†µí™” ì‹œì‘ API í˜¸ì¶œ ì™„ë£Œ');
                } catch (apiError) {
                    console.error('âŒ í†µí™” ì‹œì‘ API ì‹¤íŒ¨:', apiError.message);
                    alert(apiError.message || 'í†µí™”ë¥¼ ì‹œì‘í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.');
                    navigate('/app/home');
                    return;
                }

                // 2. ë§ˆì´í¬ ê¶Œí•œ ìš”ì²­
                const stream = await navigator.mediaDevices.getUserMedia({
                    audio: {
                        echoCancellation: true,
                        noiseSuppression: true,
                        autoGainControl: true,
                        sampleRate: 16000,
                    },
                });
                console.log('ğŸ¤ ë§ˆì´í¬ íšë“');

                if (!audioContextRef.current) {
                    audioContextRef.current = new (window.AudioContext || window.webkitAudioContext)({
                        sampleRate: 16000,
                    });
                }

                await new Promise((resolve) => setTimeout(resolve, 100));

                const source = audioContextRef.current.createMediaStreamSource(stream);
                const processor = audioContextRef.current.createScriptProcessor(VAD_CONFIG.frameSize, 1, 1);

                // VAD ìƒíƒœ ì´ˆê¸°í™”
                vadStateRef.current = {
                    isSpeaking: false,
                    silenceFrames: 0,
                    speechFrames: 0,
                    preRollBuffer: [],
                    isSending: false,
                    audioChunks: [],
                };

                processor.onaudioprocess = (e) => {
                    const socket = getAiSocket();
                    if (!socket || socket.readyState !== WebSocket.OPEN) {
                        return;
                    }

                    const inputData = e.inputBuffer.getChannelData(0);
                    const int16Data = new Int16Array(inputData.length);

                    for (let i = 0; i < inputData.length; i++) {
                        const s = Math.max(-1, Math.min(1, inputData[i]));
                        int16Data[i] = s < 0 ? s * 0x8000 : s * 0x7fff;
                    }

                    // í”„ë¦¬ë¡¤ ë²„í¼ì— ì¶”ê°€
                    vadStateRef.current.preRollBuffer.push(int16Data.buffer);
                    if (vadStateRef.current.preRollBuffer.length > VAD_CONFIG.preRollFrames) {
                        vadStateRef.current.preRollBuffer.shift();
                    }

                    // VADë¡œ ìŒì„± ê°ì§€
                    const hasSpeech = detectSpeech(inputData);
                    if (hasSpeech) {
                        vadStateRef.current.speechFrames++;
                        vadStateRef.current.silenceFrames = 0;

                        // ìŒì„± ì‹œì‘ ê°ì§€
                        if (
                            !vadStateRef.current.isSpeaking &&
                            vadStateRef.current.speechFrames >= VAD_CONFIG.speechStartFrames
                        ) {
                            vadStateRef.current.isSpeaking = true;
                            vadStateRef.current.isSending = true;
                            vadStateRef.current.audioChunks = [];

                            console.log('ğŸ¤ ìŒì„± ì‹œì‘ ê°ì§€ - start ì´ë²¤íŠ¸ ì „ì†¡');
                            setIsUserTalking(true);

                            socket.send(
                                JSON.stringify({
                                    type: 'start',
                                    lang: 'ko',
                                })
                            );

                            // í”„ë¦¬ë¡¤ ë²„í¼ í¬í•¨í•´ì„œ ì „ì†¡ ì‹œì‘
                            for (const chunk of vadStateRef.current.preRollBuffer) {
                                vadStateRef.current.audioChunks.push(chunk);
                                socket.send(chunk);
                            }
                        }

                        // ìŒì„± ì¤‘ì´ë©´ ê³„ì† ì „ì†¡
                        if (vadStateRef.current.isSpeaking && vadStateRef.current.isSending) {
                            vadStateRef.current.audioChunks.push(int16Data.buffer);
                            socket.send(int16Data.buffer);
                        }
                    } else {
                        vadStateRef.current.speechFrames = 0;

                        if (vadStateRef.current.isSpeaking) {
                            vadStateRef.current.silenceFrames++;

                            // ë¬´ìŒ ì¤‘ì—ë„ ê³„ì† ì „ì†¡
                            if (vadStateRef.current.isSending) {
                                vadStateRef.current.audioChunks.push(int16Data.buffer);
                                socket.send(int16Data.buffer);
                            }

                            // ìŒì„± ì¢…ë£Œ ê°ì§€
                            if (vadStateRef.current.silenceFrames >= VAD_CONFIG.silenceEndFrames) {
                                if (vadStateRef.current.audioChunks.length >= VAD_CONFIG.minSpeechFrames) {
                                    console.log(
                                        `ğŸ¤ ìŒì„± ì¢…ë£Œ ê°ì§€ - stop ì´ë²¤íŠ¸ ì „ì†¡ (${vadStateRef.current.audioChunks.length} í”„ë ˆì„)`
                                    );

                                    socket.send(
                                        JSON.stringify({
                                            type: 'stop',
                                        })
                                    );
                                } else {
                                    console.log(
                                        `ğŸ¤ ë„ˆë¬´ ì§§ì€ ìŒì„± - ë¬´ì‹œ (${vadStateRef.current.audioChunks.length} í”„ë ˆì„)`
                                    );
                                }

                                vadStateRef.current.isSpeaking = false;
                                vadStateRef.current.isSending = false;
                                vadStateRef.current.silenceFrames = 0;
                                vadStateRef.current.speechFrames = 0;
                                vadStateRef.current.audioChunks = [];
                                setIsUserTalking(false);
                            }
                        }
                    }
                };

                source.connect(processor);
                processor.connect(audioContextRef.current.destination);
                audioStreamRef.current = stream;
                audioSourceRef.current = source;
                audioProcessorRef.current = processor;
                setIsCallActive(true);
                console.log('âœ… í†µí™” ì‹œì‘ (ë¡œì»¬ VAD í™œì„±í™”)');
            } catch (error) {
                console.error('âŒ ë§ˆì´í¬ ì˜¤ë¥˜:', error);
                alert('ë§ˆì´í¬ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤.');
                navigate('/app/home');
            }
        };

        initCall();
    }, [location.state, character, politeness, navigate]);

    // WebSocket ë©”ì‹œì§€ ë¦¬ìŠ¤ë„ˆ
    useEffect(() => {
        if (location.state) {
            const { character, politeness } = location.state;
            // í†µí™” ì‹œì‘ API í˜¸ì¶œ
            startCall(character, politeness);
            setIsCallActive(true);
            setCurrentSubtitle('í†µí™”ê°€ ì‹œì‘ë˜ì—ˆìŠµë‹ˆë‹¤. ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë§ì”€í•´ì£¼ì„¸ìš”.');
        }
    }, [location.state]);

    // isTalking ìƒíƒœì— ë”°ë¼ video ì¬ìƒ/ì •ì§€
    useEffect(() => {
        if (!videoRef.current) return;

        if (isTalking && !isUserTalking) {
            videoRef.current.play().catch((e) => {
                console.log('Video play failed:', e);
            });
        } else {
            videoRef.current.pause();
        }
    }, [isTalking, isUserTalking]);

    const handleEndCall = useCallback(() => {
        console.log('ğŸ“´ í†µí™” ì¢…ë£Œ');

        // VAD ìƒíƒœ ì´ˆê¸°í™”
        vadStateRef.current = {
            isSpeaking: false,
            silenceFrames: 0,
            speechFrames: 0,
            preRollBuffer: [],
            isSending: false,
            audioChunks: [],
        };

        if (audioProcessorRef.current) {
            audioProcessorRef.current.disconnect();
            audioProcessorRef.current = null;
        }
        if (audioSourceRef.current) {
            audioSourceRef.current.disconnect();
            audioSourceRef.current = null;
        }
        if (audioStreamRef.current) {
            audioStreamRef.current.getTracks().forEach((track) => track.stop());
            audioStreamRef.current = null;
        }

        endCall();
        setIsTalking(false);
        setIsCallActive(false);
        navigate('/app/home');
    }, [navigate]);

    // í˜ì´ì§€ ì–¸ë¡œë“œ ì‹œ ì •ë¦¬
    useEffect(() => {
        const socket = getAiSocket();
        if (!socket) return;

        socket.onmessage = async (event) => {
            const data = event.data;

            // ğŸ§ 1) ì˜¤ë””ì˜¤ Blob/ArrayBuffer ë©”ì‹œì§€ ì²˜ë¦¬ (TTS)
            if (data instanceof Blob || data instanceof ArrayBuffer) {
                console.log('ğŸµ AI ì˜¤ë””ì˜¤ ìˆ˜ì‹ :', data);
                handleTTSAudio(data);
                return;
            }

            // ğŸ“ 2) JSON í…ìŠ¤íŠ¸ ë©”ì‹œì§€ ì²˜ë¦¬
            try {
                const msg = JSON.parse(data);
                console.log('ğŸ“© AI JSON ë©”ì‹œì§€ ìˆ˜ì‹ :', msg);
                handleWebSocketMessage(msg);
            } catch (err) {
                console.warn('âš  JSON íŒŒì‹± ì‹¤íŒ¨ ë©”ì‹œì§€:', data);
            }
        };

        // ì—ëŸ¬ ë° ì—°ê²° ì¢…ë£Œ ì²˜ë¦¬
        socket.onerror = (error) => {
            console.error('âŒ WebSocket ì˜¤ë¥˜:', error);
            toast({
                title: 'WebSocket ì˜¤ë¥˜',
                description: 'ì„œë²„ ì—°ê²°ì— ë¬¸ì œê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                status: 'error',
                duration: 3000,
            });
        };

        socket.onclose = () => {
            console.log('ğŸ”Œ WebSocket ì—°ê²° ì¢…ë£Œ');
            setIsCallActive(false);
        };

        return () => {
            // cleanup
        };
    }, [toast]);

    // WebSocket ë©”ì‹œì§€ ì²˜ë¦¬ í•¨ìˆ˜
    const handleWebSocketMessage = (msg) => {
        const { type, text, message } = msg;

        switch (type) {
            case 'stt_result':
                // STT ê²°ê³¼ (ì‚¬ìš©ì ìŒì„±ì¸ì‹ ê²°ê³¼)
                console.log('ğŸ‘¤ ì‚¬ìš©ì ë°œí™”:', text);
                setUserText(text);
                setCurrentSubtitle(`ë‚˜: ${text}`);
                setIsUserTalking(false);
                break;

            case 'stt_status':
                // STT ì²˜ë¦¬ ì¤‘
                console.log('ğŸ¤ STT ì²˜ë¦¬:', message);
                setCurrentSubtitle(message || 'STT ì²˜ë¦¬ ì¤‘...');
                break;

            case 'tts_start':
                // TTS ì‹œì‘ (AI ì‘ë‹µ í…ìŠ¤íŠ¸)
                console.log('ğŸ¤– AI ì‘ë‹µ:', text);
                setAiText(text);
                setCurrentSubtitle(text);
                setIsTalking(true);
                break;

            case 'tts_end':
                // TTS ì¢…ë£Œ
                console.log('ğŸ”Š TTS ì¬ìƒ ì™„ë£Œ');
                setIsTalking(false);
                setCurrentSubtitle('ë§ˆì´í¬ ë²„íŠ¼ì„ ëˆŒëŸ¬ ë§ì”€í•´ì£¼ì„¸ìš”.');
                break;

            case 'ready':
                // ë…¹ìŒ ì¤€ë¹„ ì™„ë£Œ
                if (msg.event === 'start') {
                    console.log('âœ… ë…¹ìŒ ì¤€ë¹„ ì™„ë£Œ');
                    setIsUserTalking(true);
                    setCurrentSubtitle('ë“£ê³  ìˆìŠµë‹ˆë‹¤...');
                }
                break;

            case 'error':
                // ì—ëŸ¬ ë©”ì‹œì§€
                console.error('âŒ ì„œë²„ ì˜¤ë¥˜:', message);
                toast({
                    title: 'ì˜¤ë¥˜ ë°œìƒ',
                    description: message || 'ì•Œ ìˆ˜ ì—†ëŠ” ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.',
                    status: 'error',
                    duration: 3000,
                });
                setIsTalking(false);
                setIsUserTalking(false);
                break;

            case 'call_summary':
                // í†µí™” ìš”ì•½
                console.log('ğŸ“Š í†µí™” ìš”ì•½:', msg);
                break;

            case 'auto_disconnect':
                // ìë™ ì¢…ë£Œ
                console.log('âš ï¸ ìë™ ì¢…ë£Œ:', message);
                toast({
                    title: 'í†µí™” ì¢…ë£Œ',
                    description: message || 'í†µí™”ê°€ ìë™ìœ¼ë¡œ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.',
                    status: 'warning',
                    duration: 3000,
                });
                handleEndCall();
                break;

            default:
                console.log('ğŸ“¨ ê¸°íƒ€ ë©”ì‹œì§€:', msg);
        }
    };

    // TTS ì˜¤ë””ì˜¤ ì¬ìƒ ì²˜ë¦¬
    const handleTTSAudio = async (audioData) => {
        try {
            // Blob ë˜ëŠ” ArrayBufferë¥¼ Blobìœ¼ë¡œ ë³€í™˜
            const blob = audioData instanceof Blob ? audioData : new Blob([audioData], { type: 'audio/wav' });

            // Blob URL ìƒì„±
            const url = URL.createObjectURL(blob);

            // ì˜¤ë””ì˜¤ ì¬ìƒ
            if (audioRef.current) {
                audioRef.current.pause();
            }

            const audio = new Audio(url);
            audioRef.current = audio;

            audio.onplay = () => {
                console.log('ğŸ”Š TTS ì˜¤ë””ì˜¤ ì¬ìƒ ì‹œì‘');
                setIsTalking(true);
            };

            audio.onended = () => {
                console.log('âœ… TTS ì˜¤ë””ì˜¤ ì¬ìƒ ì™„ë£Œ');
                setIsTalking(false);
                URL.revokeObjectURL(url);
            };

            audio.onerror = (e) => {
                console.error('âŒ TTS ì˜¤ë””ì˜¤ ì¬ìƒ ì˜¤ë¥˜:', e);
                setIsTalking(false);
                URL.revokeObjectURL(url);
            };

            await audio.play();
        } catch (error) {
            console.error('âŒ TTS ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì˜¤ë¥˜:', error);
            setIsTalking(false);
        }
    };

    // ë…¹ìŒ ì—ëŸ¬ ì²˜ë¦¬
    useEffect(() => {
        if (recordError) {
            toast({
                title: 'ë…¹ìŒ ì˜¤ë¥˜',
                description: recordError,
                status: 'error',
                duration: 3000,
            });
        }
    }, [recordError, toast]);

    // ë…¹ìŒ ë²„íŠ¼ í´ë¦­ í•¸ë“¤ëŸ¬
    const handleRecordClick = async () => {
        const socket = getAiSocket();
        if (!socket || socket.readyState !== WebSocket.OPEN) {
            toast({
                title: 'WebSocket ì—°ê²° ì•ˆ ë¨',
                description: 'ì„œë²„ì— ì—°ê²°ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.',
                status: 'error',
                duration: 3000,
            });
            return;
        }

        if (isTalking) {
            toast({
                title: 'AIê°€ ë§í•˜ëŠ” ì¤‘',
                description: 'AIê°€ ë§ì„ ë§ˆì¹  ë•Œê¹Œì§€ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”.',
                status: 'warning',
                duration: 2000,
            });
            return;
        }

        try {
            await toggleRecording(socket);
        } catch (error) {
            console.error('ë…¹ìŒ í† ê¸€ ì˜¤ë¥˜:', error);
        }
    };

    // í†µí™” ì¢…ë£Œ í•¸ë“¤ëŸ¬
    const handleEndCall = () => {
        // ë…¹ìŒ ì¤‘ì´ë©´ ì¤‘ì§€
        if (isRecording) {
            const socket = getAiSocket();
            if (socket) {
                toggleRecording(socket);
            }
        }

        // TTS ì˜¤ë””ì˜¤ ì¤‘ì§€
        if (audioRef.current) {
            audioRef.current.pause();
            audioRef.current = null;
        }

        // í†µí™” ì¢…ë£Œ API í˜¸ì¶œ
        endCall();
        setIsTalking(false);
        setIsCallActive(false);
        navigate('/app/home'); // MainPageë¡œ ëŒì•„ê°€ê¸°
    };

    return (
        <Flex minH="100vh" align="center" justify="center" bg={isHighContrast ? '#000000' : 'white'} px={3}>
            {/* ë©”ì¸ ë¡œê·¸ì¸ ì¹´ë“œ */}
            <Box p={{ base: 5, md: 14 }} w="full" maxW="530px">
                <VStack spacing={6} align="stretch">
                    {/* ìºë¦­í„° ì˜ì—­ */}
                    <MotionBox
                        initial={{ scale: 0.8, opacity: 0 }}
                        animate={{ scale: 1, opacity: 1 }}
                        transition={{ duration: 0.5 }}
                        w="100%"
                        h="450px"
                        display="flex"
                        alignItems="center"
                        justifyContent="center"
                        overflow="hidden"
                        borderRadius="15px"
                        position="relative"
                    >
                        {/* video íƒœê·¸ë¡œ webm ì¬ìƒ ì œì–´ */}
                        <Box
                            as="video"
                            ref={videoRef}
                            src={character.characterType === 'dabok' ? DabokVideo : DajeongVideo}
                            loop
                            muted
                            playsInline
                            w="100%"
                            h="70%"
                            objectFit="cover"
                            onError={(e) => {
                                console.error('Video ë¡œë“œ ì‹¤íŒ¨:', e.target.src);
                            }}
                        />

                        {/* ìƒíƒœ í‘œì‹œ ë°°ì§€ */}
                        <HStack position="absolute" top="15px" right="15px" spacing={2}>
                            {isUserTalking && (
                                <Badge colorScheme="blue" fontSize="md" px={3} py={1} borderRadius="full">
                                    ğŸ¤ ë§í•˜ëŠ” ì¤‘
                                </Badge>
                            )}
                            {isTalking && (
                                <Badge colorScheme="green" fontSize="md" px={3} py={1} borderRadius="full">
                                    ğŸ”Š AI ì‘ë‹µ ì¤‘
                                </Badge>
                            )}
                        </HStack>
                    </MotionBox>

                    {/* í˜„ì¬ ìë§‰ */}
                    <Box mt={2}>
                        <AnimatePresence mode="wait">
                            <MotionText
                                key={currentSubtitle}
                                initial={{ opacity: 0, y: 20 }}
                                animate={{ opacity: 1, y: 0 }}
                                exit={{ opacity: 0, y: -20 }}
                                transition={{ duration: 0.3 }}
                                fontSize={fs}
                                fontWeight="700"
                                color={isHighContrast ? '#FFFFFF' : '#000000'}
                                textAlign="center"
                                py={5}
                                borderRadius="15px"
                                minH="90px"
                                display="flex"
                                alignItems="center"
                                justifyContent="center"
                                w="full"
                            >
                                {currentSubtitle}
                            </MotionText>
                        </AnimatePresence>
                    </Box>

                    {/* ë…¹ìŒ ë²„íŠ¼ */}
                    <Flex justifyContent="center" mt={4}>
                        <IconButton
                            icon={isRecording ? <FaStop /> : <FaMicrophone />}
                            onClick={handleRecordClick}
                            size="lg"
                            w="80px"
                            h="80px"
                            borderRadius="50%"
                            bg={isRecording ? '#F44336' : isHighContrast ? '#FFD700' : character.color || '#2196F3'}
                            color={isHighContrast ? '#000000' : 'white'}
                            border={isHighContrast ? '3px solid white' : 'none'}
                            boxShadow={
                                isRecording ? '0 0 20px rgba(244, 67, 54, 0.6)' : '0 4px 14px rgba(33, 150, 243, 0.3)'
                            }
                            _hover={{
                                transform: 'scale(1.1)',
                                boxShadow: isRecording
                                    ? '0 0 30px rgba(244, 67, 54, 0.8)'
                                    : '0 6px 20px rgba(33, 150, 243, 0.5)',
                            }}
                            _active={{
                                transform: 'scale(0.95)',
                            }}
                            transition="all 0.2s"
                            animation={isRecording ? 'pulse 1.5s infinite' : 'none'}
                            aria-label={isRecording ? 'ë…¹ìŒ ì¤‘ì§€' : 'ë…¹ìŒ ì‹œì‘'}
                            isDisabled={!isCallActive || isTalking}
                        />
                    </Flex>

                    {/* ë…¹ìŒ ìƒíƒœ í‘œì‹œ */}
                    {isRecording && (
                        <MotionText
                            initial={{ opacity: 0 }}
                            animate={{ opacity: 1 }}
                            exit={{ opacity: 0 }}
                            fontSize="sm"
                            color={isHighContrast ? '#FFFFFF' : '#F44336'}
                            textAlign="center"
                            mt={2}
                            fontWeight="600"
                        >
                            ğŸ”´ ë…¹ìŒ ì¤‘...
                        </MotionText>
                    )}

                    {/* <Box
                        bg="white"
                        borderRadius="10px"
                        p={3}
                        h="200px"
                        overflowY="auto"
                        mt={4}
                        boxShadow="0 0 10px rgba(0,0,0,0.1)"
                    >
                        {aiMessages.map((m, idx) => (
                            <Text key={idx} color="black" mb={2}>
                                ğŸ‘‰ {m.message || JSON.stringify(m)}
                            </Text>
                        ))}
                    </Box> */}

                    {/* í†µí™” ì¢…ë£Œ ë²„íŠ¼ */}
                    <Button
                        w="full"
                        bg={isHighContrast ? '#FFD700' : '#F44336'}
                        color={isHighContrast ? '#000000' : 'white'}
                        onClick={handleEndCall}
                        fontSize={fs}
                        fontWeight="700"
                        height={callBtnH}
                        borderRadius="15px"
                        border={isHighContrast ? '3px solid white' : 'none'}
                        boxShadow="0 4px 14px rgba(244, 67, 54, 0.3)"
                        mt={2}
                        _hover={{
                            bg: isHighContrast ? '#FFEB3B' : '#D32F2F',
                            transform: 'translateY(-2px)',
                            boxShadow: isHighContrast
                                ? '0 6px 20px rgba(255, 215, 0, 0.4)'
                                : '0 6px 20px rgba(244, 67, 54, 0.4)',
                        }}
                        _active={{
                            bg: isHighContrast ? '#FFC107' : '#C62828',
                            transform: 'translateY(0)',
                        }}
                        transition="all 0.2s"
                    >
                        í†µí™” ì¢…ë£Œ
                    </Button>
                </VStack>
            </Box>
        </Flex>
    );
}
